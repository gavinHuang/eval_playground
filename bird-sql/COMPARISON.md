# Text-to-SQL Implementation Comparison

This document compares the three text-to-SQL implementations in this evaluation framework.

## Quick Overview

| Aspect | Vanilla Text2SQL | LangChain DB Agent | Snowflake Cortex Analyst |
|--------|------------------|-------------------|-------------------------|
| **Approach** | Direct prompting | Agent with tools | Managed API |
| **Complexity** | Low | Medium | Low |
| **Setup** | Database only | Database only | Snowflake + Semantic Model |
| **LLM Calls** | 1 | Multiple (agent loop) | N/A (managed) |
| **Transparency** | High | Medium | Low |
| **Customization** | High | High | Limited |
| **Token Usage** | High (schema + samples) | Variable | N/A |
| **Speed** | Fast (1 call) | Slower (multiple calls) | Depends on service |

## 1. Vanilla Text2SQL

### Architecture
```
User Question ‚Üí [Schema + Sample Data + Prompt] ‚Üí LLM ‚Üí SQL Query
```

### How It Works
1. Extract full schema (tables, columns, types, constraints)
2. Sample 3 rows from each table
3. Build comprehensive prompt with schema, samples, and question
4. Single LLM call to generate SQL
5. Parse and validate the response

### Strengths
‚úÖ **Simplicity**: Single file, straightforward implementation
‚úÖ **Transparency**: Clear prompt, no hidden logic
‚úÖ **Rich Context**: Schema + sample data helps LLM understand data
‚úÖ **Fast**: One LLM call, no iterations
‚úÖ **Easy to Debug**: Can inspect exact prompt sent to LLM
‚úÖ **Customizable**: Easy to modify prompt template

### Weaknesses
‚ö†Ô∏è **No Self-Correction**: Can't retry or fix errors
‚ö†Ô∏è **Token Heavy**: Sends full schema + samples every time
‚ö†Ô∏è **Static Context**: Can't dynamically fetch additional info
‚ö†Ô∏è **No Validation Loop**: Generates query once, doesn't verify

### Best For
- Simple to moderate complexity queries
- When you want full control over prompting
- Baseline comparisons
- Quick prototyping
- Educational purposes

### Code Example
```python
text2sql = VanillaText2SQL(db_path="debit_card.db")
result = text2sql.query("How many EUR customers?")
# Single LLM call with full context
```

## 2. LangChain DB Agent

### Architecture
```
User Question ‚Üí Agent ‚Üí [Plan] ‚Üí Tools (query/schema/list) ‚Üí [Observe] ‚Üí SQL
                  ‚Üë                                                    ‚Üì
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Iterate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### How It Works
1. Agent receives question
2. Plans which tools to use
3. Calls tools (sql_db_schema, sql_db_list_tables, sql_db_query)
4. Observes results
5. Iterates until final answer
6. Returns SQL and answer

### Strengths
‚úÖ **Self-Correcting**: Can retry queries if errors occur
‚úÖ **Dynamic**: Fetches only needed schema information
‚úÖ **Flexible**: Can explore database, check results, refine
‚úÖ **Token Efficient**: Only loads relevant schema
‚úÖ **Robust**: Handles complex multi-step reasoning
‚úÖ **Built-in Tools**: Schema inspection, query execution, validation

### Weaknesses
‚ö†Ô∏è **Complexity**: Multiple LLM calls, harder to debug
‚ö†Ô∏è **Slower**: Agent loop adds latency
‚ö†Ô∏è **Non-Deterministic**: Can take different paths
‚ö†Ô∏è **Higher Cost**: Multiple LLM calls per query
‚ö†Ô∏è **Opaque**: Hard to predict exact behavior

### Best For
- Complex queries requiring exploration
- When schema is large and token efficiency matters
- Production systems needing robustness
- Queries that might need refinement

### Code Example
```python
agent = LangChainDBAgent(db_path="debit_card.db")
result = agent.query("How many EUR customers?")
# Multiple LLM calls with tool iterations
```

## 3. Snowflake Cortex Analyst

### Architecture
```
User Question ‚Üí REST API ‚Üí Snowflake Managed Service ‚Üí SQL + Answer
                              (Semantic Model)
```

### How It Works
1. Send question + semantic model reference to API
2. Snowflake's managed service processes request
3. Uses semantic model (business definitions, relationships)
4. Returns SQL and natural language answer
5. Provides confidence scores

### Strengths
‚úÖ **Semantic Understanding**: Business logic in semantic model
‚úÖ **Managed**: No LLM infrastructure needed
‚úÖ **Optimized**: Snowflake-specific optimizations
‚úÖ **Confidence Scores**: Quality indicators
‚úÖ **Business Context**: Non-technical users can define semantics
‚úÖ **No Token Costs**: Managed service pricing

### Weaknesses
‚ö†Ô∏è **Snowflake Only**: Requires Snowflake account
‚ö†Ô∏è **Semantic Model Required**: Must create and maintain YAML
‚ö†Ô∏è **Less Control**: Can't modify underlying logic
‚ö†Ô∏è **API Dependency**: Network latency, availability
‚ö†Ô∏è **Limited Transparency**: Black box processing

### Best For
- Snowflake users
- Business intelligence applications
- When semantic layer already exists
- Non-technical user queries

### Code Example
```python
client = CortexAnalystWrapper(
    account_url="https://account.snowflake.com",
    token=token,
    semantic_model_file="@stage/model.yaml"
)
result = client.query("How many EUR customers?")
# Managed service handles everything
```

## Detailed Comparison

### Schema Handling

**Vanilla Text2SQL:**
- Extracts full schema at initialization
- Includes all tables, columns, types, constraints
- Sends everything in every prompt
- Pro: Complete context
- Con: Token intensive

**LangChain DB Agent:**
- Dynamically queries schema as needed
- Uses `sql_db_schema` tool for specific tables
- Only loads relevant portions
- Pro: Token efficient
- Con: Might miss relationships

**Cortex Analyst:**
- Uses pre-defined semantic model
- Includes business logic, relationships, definitions
- Must be maintained separately
- Pro: Business-friendly
- Con: Extra maintenance

### Sample Data

**Vanilla Text2SQL:**
- ‚úÖ Automatically includes 3 rows per table
- Helps LLM understand data format and values
- Increases context quality

**LangChain DB Agent:**
- ‚ö†Ô∏è Can query sample data via tools
- Not included by default
- Agent must decide to fetch samples

**Cortex Analyst:**
- ‚ùå No sample data in semantic model
- Relies on column definitions and types

### Error Handling

**Vanilla Text2SQL:**
```python
# Single attempt, no retry
try:
    sql = llm.generate(prompt)
    result = execute(sql)
except:
    return error
```

**LangChain DB Agent:**
```python
# Agent can retry and self-correct
try:
    sql = llm.generate()
    result = execute(sql)
except SQLError:
    # Agent observes error
    # Generates new SQL
    sql_v2 = llm.generate_with_error_context()
    result = execute(sql_v2)
```

**Cortex Analyst:**
```python
# Managed service handles errors internally
# Returns confidence score
result = api.query(question)
if result.confidence < threshold:
    # Handle low confidence
```

### Prompt Engineering

**Vanilla Text2SQL:**
- Full control over prompt template
- Easy to modify instructions
- Can add domain-specific guidance
- Example:
```python
template = """
DATABASE SCHEMA:
{schema}

SAMPLE DATA:
{samples}

Generate SQLite query for: {question}
"""
```

**LangChain DB Agent:**
- System prompt + tool descriptions
- Less direct control
- Can customize system message
- Agent decides tool usage

**Cortex Analyst:**
- No direct prompt control
- Guidance through semantic model
- Business definitions replace technical prompts

## Performance Considerations

### Latency

**Vanilla Text2SQL:**
- ‚ö° Fastest: Single LLM call
- ~1-2 seconds typical
- Depends on model and context size

**LangChain DB Agent:**
- üê¢ Slowest: Multiple LLM calls
- ~3-5+ seconds typical
- Variable based on agent iterations

**Cortex Analyst:**
- ‚ö° Fast: Optimized managed service
- ~1-3 seconds typical
- Depends on network and service load

### Cost

**Vanilla Text2SQL:**
- üí∞ Moderate: High tokens per call (schema + samples)
- Single call keeps cost predictable
- Example: ~2000 tokens input, ~100 tokens output

**LangChain DB Agent:**
- üí∞üí∞ Higher: Multiple smaller calls
- Variable cost based on iterations
- Example: 3-5 calls √ó ~500 tokens each

**Cortex Analyst:**
- üí∞ Different model: Service pricing, not token-based
- Predictable cost per query
- No token counting needed

### Accuracy

Depends heavily on:
- Query complexity
- Schema clarity
- Model capability
- Sample data quality

**General trends** (from evaluations):
1. **Cortex Analyst**: Highest (semantic model helps)
2. **LangChain Agent**: Medium-High (self-correction helps)
3. **Vanilla Text2SQL**: Medium (single-shot limits)

## When to Use Each

### Choose Vanilla Text2SQL When:
- ‚úÖ Building a baseline or prototype
- ‚úÖ You need full control over prompting
- ‚úÖ Schema is small (fits in context easily)
- ‚úÖ Queries are simple to moderate complexity
- ‚úÖ You want transparency and debuggability
- ‚úÖ Single-shot generation is sufficient

### Choose LangChain DB Agent When:
- ‚úÖ Queries are complex and exploratory
- ‚úÖ Schema is large (need selective loading)
- ‚úÖ Self-correction and robustness are important
- ‚úÖ You can tolerate higher latency
- ‚úÖ You want built-in error recovery
- ‚úÖ Working with any SQL database (SQLite, Postgres, MySQL, etc.)

### Choose Cortex Analyst When:
- ‚úÖ You're already using Snowflake
- ‚úÖ You have or need a semantic layer
- ‚úÖ Business users define query semantics
- ‚úÖ You want managed infrastructure
- ‚úÖ Confidence scores are valuable
- ‚úÖ You prefer API-based solutions

## Migration Path

### From Vanilla ‚Üí LangChain Agent
```python
# Vanilla
text2sql = VanillaText2SQL(db_path="db.db")

# To LangChain
agent = LangChainDBAgent(db_path="db.db")
# Interface is the same!
```

### From LangChain ‚Üí Cortex Analyst
1. Create semantic model YAML
2. Upload to Snowflake stage
3. Switch to API client
```python
# LangChain
agent = LangChainDBAgent(db_path="db.db")

# To Cortex
client = CortexAnalystWrapper(
    account_url=url,
    token=token,
    semantic_model_file="@stage/model.yaml"
)
# Interface is the same!
```

## Hybrid Approaches

You can combine approaches:

### Vanilla + Validation
```python
# Generate with vanilla
result = vanilla.query(question)
sql = result['sql']

# Validate with agent
validation = agent.validate_sql(sql)
if not validation['valid']:
    sql = agent.query(question)['sql']
```

### Agent + Semantic Context
```python
# Load semantic definitions
semantics = load_semantic_model()

# Inject into agent prompt
agent = LangChainDBAgent(
    db_path="db.db",
    system_prompt=f"Use these definitions: {semantics}"
)
```

## Conclusion

Each approach has its place:

- **Vanilla Text2SQL**: Best for learning, prototyping, and simple use cases
- **LangChain DB Agent**: Best for production, complex queries, and robustness
- **Cortex Analyst**: Best for Snowflake users wanting managed solutions

The unified interface in this framework makes it easy to evaluate and compare all three!
